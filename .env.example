# =============================================================================
# DIABETES PREDICTION PIPELINE - ENVIRONMENT CONFIGURATION TEMPLATE
# =============================================================================
# Project: Big Data Pipeline for Diabetes Prediction
# Team: Kelompok 8 RA
# Environment: Development/Production
# Last Updated: May 26, 2025
# =============================================================================
# 
# INSTRUCTIONS:
# 1. Copy this file to .env
# 2. Replace all placeholder values with your actual configuration
# 3. Never commit the .env file to version control
# =============================================================================

# -----------------------------------------------------------------------------
# PROJECT CONFIGURATION
# -----------------------------------------------------------------------------
PROJECT_NAME=diabetes-prediction-pipeline
PROJECT_VERSION=1.0.0
ENVIRONMENT=development  # development|staging|production
COMPOSE_PROJECT_NAME=diabetes-pipeline

# -----------------------------------------------------------------------------
# NETWORK CONFIGURATION
# -----------------------------------------------------------------------------
NETWORK_NAME=diabetes-network
NETWORK_DRIVER=bridge

# -----------------------------------------------------------------------------
# HADOOP HDFS CONFIGURATION
# -----------------------------------------------------------------------------
HADOOP_VERSION=3.2.1
HDFS_NAMENODE_HOST=namenode
HDFS_NAMENODE_PORT=9000
HDFS_NAMENODE_HTTP_PORT=9870
HDFS_DATANODE_HTTP_PORT=9864
HDFS_REPLICATION_FACTOR=1
HDFS_BLOCK_SIZE=134217728
CLUSTER_NAME=diabetes-cluster

# Core Hadoop Configuration
CORE_CONF_fs_defaultFS=hdfs://namenode:9000
CORE_CONF_hadoop_http_staticuser_user=root
CORE_CONF_hadoop_proxyuser_hue_hosts=*
CORE_CONF_hadoop_proxyuser_hue_groups=*
CORE_CONF_io_compression_codecs=org.apache.hadoop.io.compress.SnappyCodec

# HDFS Configuration
HDFS_CONF_dfs_webhdfs_enabled=true
HDFS_CONF_dfs_permissions_enabled=false
HDFS_CONF_dfs_namenode_datanode_registration_ip___hostname___check=false

# YARN Configuration
YARN_CONF_yarn_log___aggregation___enable=true
YARN_CONF_yarn_log_server_url=http://historyserver:8188/applicationhistory/logs/
YARN_CONF_yarn_resourcemanager_recovery_enabled=true
YARN_CONF_yarn_resourcemanager_store_class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
YARN_CONF_yarn_resourcemanager_scheduler_class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___mb=8192
YARN_CONF_yarn_scheduler_capacity_root_default_maximum___allocation___vcores=4

# -----------------------------------------------------------------------------
# APACHE SPARK CONFIGURATION
# -----------------------------------------------------------------------------
SPARK_VERSION=3.3.0
SPARK_MASTER_HOST=spark-master
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8081
SPARK_WORKER_WEBUI_PORT=8082
SPARK_WORKER_CORES=2
SPARK_WORKER_MEMORY=2g
SPARK_DRIVER_MEMORY=1g
SPARK_EXECUTOR_MEMORY=1g
SPARK_EXECUTOR_CORES=1

# Spark Configuration Properties
SPARK_CONF_spark_serializer=org.apache.spark.serializer.KryoSerializer
SPARK_CONF_spark_sql_adaptive_enabled=true
SPARK_CONF_spark_sql_adaptive_coalescePartitions_enabled=true
SPARK_CONF_spark_sql_adaptive_skewJoin_enabled=true
SPARK_CONF_spark_sql_warehouse_dir=hdfs://namenode:9000/user/hive/warehouse
SPARK_CONF_spark_hadoop_fs_defaultFS=hdfs://namenode:9000
SPARK_CONF_spark_eventLog_enabled=true
SPARK_CONF_spark_eventLog_dir=hdfs://namenode:9000/spark-logs
SPARK_CONF_spark_history_fs_logDirectory=hdfs://namenode:9000/spark-logs

# -----------------------------------------------------------------------------
# APACHE HIVE CONFIGURATION
# -----------------------------------------------------------------------------
HIVE_VERSION=2.3.2
HIVE_METASTORE_PORT=9083
HIVE_SERVER2_PORT=10000

# Hive Site Configuration
HIVE_SITE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore-postgresql/metastore
HIVE_SITE_CONF_javax_jdo_option_ConnectionDriverName=org.postgresql.Driver
HIVE_SITE_CONF_javax_jdo_option_ConnectionUserName=hive
HIVE_SITE_CONF_javax_jdo_option_ConnectionPassword=hive  # Change in production
HIVE_SITE_CONF_datanucleus_autoCreateSchema=false
HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083

# Hive PostgreSQL Configuration
HIVE_POSTGRES_USER=hive
HIVE_POSTGRES_PASSWORD=hive  # Change in production
HIVE_POSTGRES_DB=metastore

# -----------------------------------------------------------------------------
# APACHE AIRFLOW CONFIGURATION
# -----------------------------------------------------------------------------
AIRFLOW_VERSION=2.7.0
AIRFLOW_UID=50000
AIRFLOW_GID=0
AIRFLOW_PROJ_DIR=./airflow

# Airflow Core Configuration
AIRFLOW__CORE__FERNET_KEY=CHANGE_THIS_TO_A_SECURE_FERNET_KEY  # Generate with python cryptography
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
AIRFLOW__CORE__LOAD_EXAMPLES=false
AIRFLOW__CORE__EXECUTOR=LocalExecutor

# Airflow Database Configuration
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=false

# Airflow Authentication
AIRFLOW__WEBSERVER__AUTHENTICATE=false
AIRFLOW__WEBSERVER__RBAC=false

# Airflow Admin User
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin  # Change in production
AIRFLOW_ADMIN_EMAIL=admin@diabetes-pipeline.local

# Airflow PostgreSQL Configuration
AIRFLOW_POSTGRES_USER=airflow
AIRFLOW_POSTGRES_PASSWORD=airflow  # Change in production
AIRFLOW_POSTGRES_DB=airflow

# -----------------------------------------------------------------------------
# APACHE NIFI CONFIGURATION
# -----------------------------------------------------------------------------
NIFI_VERSION=1.18.0
NIFI_WEB_HTTP_PORT=8443
NIFI_WEB_HTTPS_PORT=8443
NIFI_CLUSTER_IS_NODE=false
NIFI_CLUSTER_NODE_PROTOCOL_PORT=8082
NIFI_ZK_CONNECT_STRING=zookeeper:2181

# NiFi Authentication (set to false for development)
NIFI_WEB_PROXY_CONTEXT_PATH=
SINGLE_USER_CREDENTIALS_USERNAME=admin
SINGLE_USER_CREDENTIALS_PASSWORD=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB  # Change in production

# -----------------------------------------------------------------------------
# ZOOKEEPER CONFIGURATION
# -----------------------------------------------------------------------------
ZOOKEEPER_VERSION=3.7.0
ZOOKEEPER_CLIENT_PORT=2181
ZOOKEEPER_TICK_TIME=2000

# -----------------------------------------------------------------------------
# MONITORING CONFIGURATION
# -----------------------------------------------------------------------------
# Prometheus
PROMETHEUS_VERSION=v2.45.0
PROMETHEUS_PORT=9090
PROMETHEUS_RETENTION_TIME=15d
PROMETHEUS_RETENTION_SIZE=10GB

# Grafana
GRAFANA_VERSION=10.0.0
GRAFANA_PORT=3000
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin  # Change in production
GRAFANA_SECURITY_ADMIN_PASSWORD=admin  # Change in production

# Node Exporter
NODE_EXPORTER_VERSION=v1.6.0
NODE_EXPORTER_PORT=9100

# -----------------------------------------------------------------------------
# MACHINE LEARNING CONFIGURATION
# -----------------------------------------------------------------------------
# MLflow
MLFLOW_VERSION=2.5.0
MLFLOW_PORT=5000
MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db  # Change to PostgreSQL in production
MLFLOW_DEFAULT_ARTIFACT_ROOT=./mlruns

# Model Configuration
MODEL_REGISTRY_PATH=./models
MODEL_ARTIFACT_PATH=./models/artifacts
MODEL_EVALUATION_METRICS_PATH=./models/metrics

# -----------------------------------------------------------------------------
# DATA CONFIGURATION
# -----------------------------------------------------------------------------
# Data Paths
DATA_BRONZE_PATH=./data/bronze
DATA_SILVER_PATH=./data/silver
DATA_GOLD_PATH=./data/gold
DATA_LOGS_PATH=./data/logs

# Data Quality Thresholds
DATA_QUALITY_NULL_THRESHOLD=0.05
DATA_QUALITY_DUPLICATE_THRESHOLD=0.01
DATA_QUALITY_OUTLIER_THRESHOLD=0.1

# -----------------------------------------------------------------------------
# SECURITY CONFIGURATION
# -----------------------------------------------------------------------------
# JWT Configuration (if implementing API authentication)
JWT_SECRET_KEY=your-super-secret-jwt-key-change-in-production
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# SSL/TLS Configuration
SSL_ENABLED=false
SSL_CERT_PATH=./certs/cert.pem
SSL_KEY_PATH=./certs/key.pem

# -----------------------------------------------------------------------------
# LOGGING CONFIGURATION
# -----------------------------------------------------------------------------
LOG_LEVEL=INFO  # DEBUG|INFO|WARNING|ERROR|CRITICAL
LOG_FORMAT=json
LOG_FILE_PATH=./logs
LOG_ROTATION_SIZE=100MB
LOG_RETENTION_DAYS=30

# -----------------------------------------------------------------------------
# PERFORMANCE CONFIGURATION
# -----------------------------------------------------------------------------
# Memory Limits (for Docker containers)
NAMENODE_MEMORY_LIMIT=2g
DATANODE_MEMORY_LIMIT=2g
SPARK_MASTER_MEMORY_LIMIT=1g
SPARK_WORKER_MEMORY_LIMIT=2g
AIRFLOW_MEMORY_LIMIT=2g
NIFI_MEMORY_LIMIT=2g

# CPU Limits
NAMENODE_CPU_LIMIT=1
DATANODE_CPU_LIMIT=1
SPARK_MASTER_CPU_LIMIT=0.5
SPARK_WORKER_CPU_LIMIT=2
AIRFLOW_CPU_LIMIT=1
NIFI_CPU_LIMIT=1

# -----------------------------------------------------------------------------
# BACKUP CONFIGURATION
# -----------------------------------------------------------------------------
BACKUP_ENABLED=true
BACKUP_SCHEDULE=0 2 * * *  # Daily at 2 AM
BACKUP_RETENTION_DAYS=7
BACKUP_PATH=./backups

# -----------------------------------------------------------------------------
# DEVELOPMENT CONFIGURATION
# -----------------------------------------------------------------------------
DEBUG_MODE=true
ENABLE_PROFILING=false
ENABLE_HOT_RELOAD=true

# Testing Configuration
TEST_DATA_PATH=./tests/data
TEST_DATABASE_URL=sqlite:///test.db

# -----------------------------------------------------------------------------
# PRODUCTION OVERRIDES
# -----------------------------------------------------------------------------
# Uncomment and modify these values for production deployment
# ENVIRONMENT=production
# DEBUG_MODE=false
# AIRFLOW__WEBSERVER__AUTHENTICATE=true
# AIRFLOW__WEBSERVER__RBAC=true
# SSL_ENABLED=true
# HDFS_REPLICATION_FACTOR=3
# MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:password@postgres:5432/mlflow

# -----------------------------------------------------------------------------
# CUSTOM CONFIGURATION
# -----------------------------------------------------------------------------
# Add your custom environment variables below
